{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/LinYichen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/LinYichen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_file = np.load('data_train.pkl', allow_pickle=True)\n",
    "data_test_file = np.load('data_test.pkl', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comment_list = data_train_file[0]\n",
    "train_topic_list = data_train_file[1]\n",
    "test_comment_list = data_test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict = {'Comment': train_comment_list, 'Topic': train_topic_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_data_dict)\n",
    "# df.drop_duplicates() # Drop duplicates\n",
    "# Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_topic_list, return_counts=True)\n",
    "len(np.unique(train_topic_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export test result labels to CSV file\n",
    "class CSVExporter:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def export(self, result_list):\n",
    "        with open('submission.csv', 'w', newline='') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            csvwriter.writerow(['Id','Category'])\n",
    "            for i in range(len(result_list)):\n",
    "                csvwriter.writerow([i, result_list[i]])\n",
    "            \n",
    "        return \"Export Success!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Classifier, randomly assign labels to comments\n",
    "class RandomClassifier:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def train(self, train_inputs, train_labels):\n",
    "        #  Not really training, but just to initiate\n",
    "        self.train_inputs = train_inputs\n",
    "        self.train_labels = train_labels\n",
    "    \n",
    "    def compute_predictions(self, test_inputs):\n",
    "        result_labels = []\n",
    "        for i in range(len(test_inputs)):\n",
    "            random_index = rd.randrange(0, len(self.train_labels) - 1, 1)\n",
    "            result_labels.append(train_labels[random_index])\n",
    "        return result_labels \n",
    "\n",
    "# Compute error rates on different classifiers    \n",
    "class ErrorRate:\n",
    "    # train_labels has to be the same size with validation_labels!\n",
    "    def __init__(self, train_inputs, train_labels, test_inputs, test_labels):\n",
    "        self.train_inputs = train_inputs\n",
    "        self.train_labels = train_labels\n",
    "        self.test_inputs = test_inputs\n",
    "        self.test_labels = test_labels\n",
    "    \n",
    "    def random_classifier(self):      \n",
    "        rc = RandomClassifier()\n",
    "        rc.train(self.train_inputs, self.train_labels)\n",
    "        result_list = rc.compute_predictions(self.test_inputs)\n",
    "        error_count = 0\n",
    "        for i in range(len(result_list)):\n",
    "            if result_list[i] != self.test_labels[i]:\n",
    "                error_count += 1\n",
    "        error_rate = error_count / len(self.test_labels)       \n",
    "        return error_rate\n",
    "        \n",
    "    def naive_bayes(self):\n",
    "        nb = NaiveBayesClassifier(self.train_inputs, self.train_labels)\n",
    "        nb.train()\n",
    "        result_list = nb.predict(self.test_inputs)\n",
    "        error_count = 0\n",
    "        for i in range(len(result_list)):\n",
    "            if result_list[i] != self.test_labels[i]:\n",
    "                error_count += 1\n",
    "        error_rate = error_count / len(self.test_labels)       \n",
    "        return error_rate\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accruracy is  0.04942857142857138\n"
     ]
    }
   ],
   "source": [
    "train_inputs = train_df.Comment # Or train_comment_list\n",
    "train_labels = train_df.Topic # Or train_topic_list\n",
    "test_inputs = test_comment_list\n",
    "\n",
    "# Random Classifier error rates\n",
    "rc = RandomClassifier()\n",
    "rc.train(train_inputs, train_labels)\n",
    "result_list = rc.compute_predictions(test_inputs)\n",
    "\n",
    "er = ErrorRate(train_inputs, train_labels, train_inputs, train_labels)\n",
    "random_classifier_error_rate = er.random_classifier()\n",
    "print('Accruracy is ', 1 - random_classifier_error_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csvexp = CSVExporter()\n",
    "# csvexp.export(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Comments processor, remove punctuations and stopwords\n",
    "def list_processor(stop_words, comment):\n",
    "    # Remove punctuations\n",
    "    comment = re.sub('[^A-Za-z0-9]+', ' ', comment)\n",
    "    # Split words into string list\n",
    "    word_tokens = word_tokenize(comment) \n",
    "    # Remove stopwords\n",
    "    word_list = [word.lower() for word in word_tokens if not word.lower() in stop_words]\n",
    "    \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, train_comment_list, train_topic_list):\n",
    "        self.train_comment_list = train_comment_list\n",
    "        self.topic_list = train_topic_list\n",
    "        self.unique_topics = np.unique(train_topic_list)\n",
    "        self.percent_dict = {}\n",
    "        self.topic_freq = []\n",
    "    \n",
    "    # Get word frequency dictionary in each topics\n",
    "    def train(self):\n",
    "        # Get topic frequency array\n",
    "        for i in range(len(self.unique_topics)):\n",
    "            topic_count = self.topic_list.count(self.unique_topics[i])\n",
    "            total_count = len(self.topic_list)\n",
    "            self.topic_freq.append(topic_count/total_count)\n",
    "        # Get Percent dictionary\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_dict = {}\n",
    "        for i, comment in enumerate(self.train_comment_list):\n",
    "            word_list = list_processor(stop_words, comment)\n",
    "            topic = self.topic_list[i]\n",
    "            topic_id = np.argwhere(self.unique_topics==topic)[0][0]\n",
    "            for word in word_list:\n",
    "                if word in word_dict:\n",
    "                    word_dict[word][topic_id] += 1\n",
    "                else:\n",
    "                    word_dict[word] = [0 for _ in range(len(self.unique_topics))]          \n",
    "                    word_dict[word][topic_id] += 1  \n",
    "        # P(w|c) = (count(w,c) + 1)/(count(c) + total words)\n",
    "        count = len(word_dict.keys())            \n",
    "        values = np.array(list(word_dict.values()))\n",
    "        sums = np.sum(values, axis = 0)\n",
    "        percents = (values + 1)/(sums + count)\n",
    "        self.percent_dict = dict(zip(word_dict.keys(), percents))\n",
    "\n",
    "    # Return: result label list    \n",
    "    def predict(self, predict_comment_list):\n",
    "        result = []\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        for comment in predict_comment_list:\n",
    "            max_percent = float(\"-inf\")\n",
    "            label = ''\n",
    "            word_list = list_processor(stop_words, comment)\n",
    "            for i in range(len(self.unique_topics)):\n",
    "                percent = np.log(self.topic_freq[i])\n",
    "                for word in word_list:\n",
    "                    if word in self.percent_dict.keys():\n",
    "                        # Use log probability to avoid under flow\n",
    "                        percent += np.log(self.percent_dict[word][i])         \n",
    "                if percent > max_percent:\n",
    "                    label = self.unique_topics[i]\n",
    "                    max_percent = percent\n",
    "            result.append(label)  \n",
    "        return result                       \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation test, split dataset into train set(60000) and validation set(10000)\n",
    "train_inputs= train_comment_list[:60000]\n",
    "train_labels= train_topic_list[:60000]\n",
    "test_inputs= train_comment_list[60000:70000]\n",
    "test_labels= train_topic_list[60000:70000]\n",
    "er = ErrorRate(train_inputs, train_labels, test_inputs, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5525\n"
     ]
    }
   ],
   "source": [
    "rate = er.naive_bayes()\n",
    "print(1-rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Export Success!'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export result label list\n",
    "nb = NaiveBayesClassifier(train_comment_list, train_topic_list)\n",
    "nb.train()\n",
    "result_list = nb.predict(test_comment_list)\n",
    "csvexp = CSVExporter()\n",
    "csvexp.export(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
